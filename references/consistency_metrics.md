# 评估一致性指标详解

## 什么是评估一致性

评估一致性（Evaluation Consistency）是指不同评估主体（人与人、人与AI、AI与AI）或同一主体在不同时间，对相同输出给出评价结论的重合程度。

### 一致性的三个层面

| 类型 | 定义 | 重要性 |
|-----|------|-------|
| 评估者内一致性 | 同一人不同时间的判断一致 | 确保标准理解稳定 |
| 评估者间一致性 | 不同人对同一样本判断一致 | 消除主观偏见 |
| 人机一致性 | AI评分与人工评分一致 | 支撑自动化评估 |

---

## 核心指标定义

### 1. 完全一致率（Exact Agreement）

**定义**：分数完全相同的样本占比

**计算**：
```
完全一致率 = 完全相同的样本数 / 总样本数 × 100%
```

**解读**：
- ≥70%：良好
- 50-70%：可接受
- <50%：需要优化

**优点**：直观易懂
**缺点**：对分数差1也判为不一致，可能过于严格

---

### 2. 相邻一致率（Adjacent Agreement）

**定义**：分数差距≤1分的样本占比

**计算**：
```
相邻一致率 = |分数差|≤1的样本数 / 总样本数 × 100%
```

**解读**：
- ≥90%：良好
- 80-90%：可接受
- <80%：需要优化

**适用场景**：允许评估者在相邻档位间有合理波动

---

### 3. Cohen's Kappa系数

**定义**：考虑随机一致性的校正指标

**计算**：
```
κ = (Po - Pe) / (1 - Pe)

Po = 实际一致率（完全一致率）
Pe = 随机一致的期望概率
```

**Pe计算（5级评分）**：
```python
# 假设两个评估者的分数分布为 p1[0-4] 和 p2[0-4]
Pe = Σ p1[i] × p2[i]  # i从0到4
```

**解读标准（Landis & Koch, 1977）**：
| Kappa值 | 一致性水平 |
|--------|-----------|
| <0.20 | 极差 |
| 0.20-0.40 | 一般 |
| 0.40-0.60 | 中等 |
| 0.60-0.80 | 较好 |
| 0.80-1.00 | 极好 |

**目标**：≥0.6（较好及以上）

**优点**：校正了随机因素，更客观
**缺点**：对分布极端不均匀的情况可能失真

---

### 4. 加权Kappa（Weighted Kappa）

**定义**：考虑不一致程度的Kappa变体

**计算**：差距越大，惩罚越重

**权重矩阵示例（线性）**：
```
     预测
实际  0    1    2    3    4
0    [1.0  0.75 0.50 0.25 0.0]
1    [0.75 1.0  0.75 0.50 0.25]
2    [0.50 0.75 1.0  0.75 0.50]
3    [0.25 0.50 0.75 1.0  0.75]
4    [0.0  0.25 0.50 0.75 1.0]
```

**适用场景**：当评分是有序量表，差2分比差1分更严重时

---

### 5. 平均绝对误差（MAE）

**定义**：平均分数差距的绝对值

**计算**：
```
MAE = Σ|人工分数 - 模型分数| / 样本数
```

**解读**：
- ≤0.5：良好（5分制）
- 0.5-1.0：可接受
- >1.0：需要优化

**优点**：直观反映平均偏差程度

---

### 6. Pearson相关系数

**定义**：两组分数的线性相关程度

**计算**：
```
r = Cov(X,Y) / (σX × σY)
```

**解读**：
- ≥0.8：强相关
- 0.6-0.8：中等相关
- <0.6：弱相关

**适用场景**：检验评分趋势是否一致

**注意**：高相关不等于高一致（可能存在系统偏移）

---

## 指标选择建议

| 场景 | 推荐指标 |
|-----|---------|
| 日常监控 | 完全一致率 + 相邻一致率 |
| 严格评估 | Cohen's Kappa |
| 有序量表 | 加权Kappa |
| 连续分数 | Pearson相关系数 + MAE |
| 综合评估 | 以上全部 |

---

## 分维度分析方法

### 按评分档位分析

检查各档位的一致性是否均衡：
```
分数档位  人工分布  模型分布  一致率
0分       3%       2%       80%
1分       8%       5%       60%
2分       15%      12%      70%
3分       64%      71%      85%
4分       10%      10%      50%
```

**发现**：4分一致率低 → 检查4分标准是否清晰

### 按内容类型分析

```
内容类型    样本数  一致率  Kappa
资讯问答    500    72%    0.65
闲聊陪伴    300    68%    0.58
情绪支持    200    61%    0.48
```

**发现**：情绪支持类一致率低 → 强化该类评估标准

### 按维度分析

```
维度        触发次数  一致率
相关性      120      85%
内容深度    200      70%
共情度      150      55%
交互引导    180      50%
```

**发现**：交互引导维度一致率最低 → 优先优化

---

## 常见一致性问题及解决

| 问题模式 | 可能原因 | 解决方案 |
|---------|---------|---------|
| 系统性偏高 | Prompt不够严格 | 强化扣分条款 |
| 系统性偏低 | 判定阈值过严 | 放宽边界条件 |
| 高分不一致 | 4分标准不清晰 | 增加4分自检清单 |
| 边界不稳定 | 2-3分区分模糊 | 增加锚点示例 |
| 特定类型误判 | 缺少针对性说明 | 补充专项条款 |

---

## 一致性提升目标设定

**阶段性目标**：

| 阶段 | 完全一致率 | Kappa | 相邻一致率 |
|-----|-----------|-------|-----------|
| 初版 | >50% | >0.4 | >75% |
| 优化版 | >65% | >0.55 | >85% |
| 成熟版 | >75% | >0.65 | >92% |

**收敛判定**：连续3轮迭代提升<1%时，考虑当前方案已接近最优
